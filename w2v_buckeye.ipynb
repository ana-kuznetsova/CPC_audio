{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import tqdm\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from sklearn.cluster import KMeans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "pathDF = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/data/word_df_fixed_nonwords.csv\" # not really necessary?\n",
    "pathLabels = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/data/converted_aligned_words_labels_fixed_nonwords.txt\"\n",
    "pathWordLabelsDict = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/data/word_labels.pickle'\n",
    "\n",
    "pathEncodingsDir = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/data/encodings/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/encoded_words\"\n",
    "\n",
    "pathKMeans_10000 = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_10000'\n",
    "pathKMeans_10500 = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_10500'\n",
    "pathKMeans_11000 = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_11000'\n",
    "\n",
    "path_w2v_10000 = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K10000\"\n",
    "path_w2v_10500 = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K10500\"\n",
    "path_w2v_11000 = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K11000\"\n",
    "\n",
    "# weighted Kmeans models\n",
    "pathKMeans_10000_weighted = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_10000_weighted_True'\n",
    "pathKMeans_10500_weighted = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_10500_weighted_True'\n",
    "pathKMeans_11000_weighted = '/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/kmeans_K_11000_weighted_True'\n",
    "\n",
    "path_w2v_10000_weighted = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K10000_weighted_True\"\n",
    "path_w2v_10500_weighted = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K10500_weighted_True\"\n",
    "path_w2v_11000_weighted = \"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/hacpc32neg-6-12-2-4-gt-cosine-norelu-encodeseg-ls/word2vec_window_5_nepoch_30_vectorsize_100_K11000_weighted_True\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "encExtension = \".pt\"\n",
    "encFiles = glob.glob(os.path.join(pathEncodingsDir, '*' + encExtension))\n",
    "encodings = []\n",
    "if encExtension == \".txt\":\n",
    "    for encfile in tqdm.tqdm(encFiles):\n",
    "        with open(encfile, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            encodings.append([float(x) for x in data])\n",
    "    wordEncs = torch.tensor(encodings)\n",
    "elif encExtension == \".pt\":\n",
    "    for encfile in tqdm.tqdm(encFiles):\n",
    "            encodings.append(torch.tensor(torch.load(encfile)))\n",
    "    wordEncs = torch.cat(encodings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 255/255 [00:08<00:00, 30.46it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "wordEncs.shape[0] == len(uniqueConsecutiveWords)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "with open(pathLabels, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "output = {}  # Step in librispeech dataset is 160bits\n",
    "maxPhone = 0\n",
    "allLabels = []\n",
    "for line in lines:\n",
    "    data = line.split()\n",
    "    recordingLabels = [int(x) for x in data[1:]]\n",
    "    output[data[0]] = recordingLabels\n",
    "    allLabels += recordingLabels\n",
    "    maxPhone = max(maxPhone, max(output[data[0]]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "file = open(pathWordLabelsDict,'rb')\n",
    "word2Labels = pickle.load(file)\n",
    "file.close()\n",
    "labels2Words = {v: k for k, v in word2Labels.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "uniqueConsecutiveLabels = [x[0] for x in groupby(allLabels)]\n",
    "len(uniqueConsecutiveLabels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "348122"
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "df = pd.read_csv(pathDF)\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(350924, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "df.shape[0] - len(uniqueConsecutiveLabels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2802"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "uniqueConsecutiveWords = list(map(labels2Words.get, uniqueConsecutiveLabels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split long list with everything into digestable shorter sentences for word2vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "n_words_in_sentence = 15\n",
    "splitWords = []\n",
    "lastIdx = 0\n",
    "for i in range(len(uniqueConsecutiveWords) // n_words_in_sentence):\n",
    "    sentence = uniqueConsecutiveWords[i * n_words_in_sentence : (i+1) * n_words_in_sentence]\n",
    "    splitWords.append(sentence)\n",
    "    lastIdx = (i+1) * n_words_in_sentence\n",
    "splitWords.append(uniqueConsecutiveWords[lastIdx:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## word2vec model trained on regular text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "text_model = Word2Vec(min_count=1, window=5, vector_size=100)\n",
    "text_model.build_vocab(corpus_iterable=splitWords)\n",
    "text_model.train(corpus_iterable=splitWords, total_examples=text_model.corpus_count, total_words=text_model.corpus_total_words, epochs=30)\n",
    "\n",
    "text_model.save(\"/pio/scratch/1/i325922/data/BUCKEYE/raw/word2vec_encodings/models/text_model/w2v.model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "investigatedWord = 'small'\n",
    "text_model.wv.most_similar(investigatedWord, topn=5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('large', 0.7316713333129883),\n",
       " ('such', 0.7254460453987122),\n",
       " ('rural', 0.6970985531806946),\n",
       " ('christian', 0.6497025489807129),\n",
       " (\"england's\", 0.6425490379333496)]"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "def get_kmeans_results(pathCheckpoint, encodings, wordLabels, preds=True):\n",
    "    with open(pathCheckpoint + '/kmeans.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    if preds:\n",
    "        labels = model.predict(encodings)\n",
    "    else:\n",
    "        labels = model.labels_\n",
    "    cluster2WordDict = {}\n",
    "    npLabels = np.array(wordLabels)\n",
    "    for i in range(10000):\n",
    "        wordsWhereCluster = (np.where(labels == i)[0])\n",
    "        cluster2WordDict[i] = list(map(labels2Words.get, npLabels[wordsWhereCluster]))\n",
    "    return model, labels, cluster2WordDict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K = 10000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "k10000, k10000_preds, k10000_cluster2WordDict = get_kmeans_results(pathKMeans_10000, wordEncs, uniqueConsecutiveLabels, preds=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "k10000_weighted, k10000_weighted_preds, k10000_weighted_cluster2WordDict = get_kmeans_results(pathKMeans_10000_weighted, wordEncs, uniqueConsecutiveLabels, preds=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "k10000_weighted_cluster2WordDict[666]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<VOCNOISE>',\n",
       " '<SIL>',\n",
       " '<SIL>',\n",
       " '<SIL>',\n",
       " 'she',\n",
       " 'comes',\n",
       " 'yknow',\n",
       " 'types',\n",
       " '<VOCNOISE>',\n",
       " '<IVER>',\n",
       " 'can',\n",
       " 'right',\n",
       " '<SIL>',\n",
       " 'it',\n",
       " '<SIL>',\n",
       " '<VOCNOISE>',\n",
       " 'got',\n",
       " '<SIL>',\n",
       " 'they',\n",
       " 'could',\n",
       " 'i',\n",
       " 'a',\n",
       " 'order',\n",
       " 'ask',\n",
       " 'independent',\n",
       " 'past',\n",
       " 'loves',\n",
       " 'that',\n",
       " 'ships',\n",
       " '<VOCNOISE>',\n",
       " 'child',\n",
       " '<IVER>',\n",
       " 'but',\n",
       " 'was',\n",
       " \"she's\",\n",
       " 'and',\n",
       " 'it',\n",
       " 'they',\n",
       " 'grade',\n",
       " 'they',\n",
       " 'it',\n",
       " 'state',\n",
       " 'when',\n",
       " 'more']"
      ]
     },
     "metadata": {},
     "execution_count": 267
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "source": [
    "k10000_cluster2WordDict[666]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['school',\n",
       " \"won't\",\n",
       " 'yknow',\n",
       " 'you',\n",
       " '<VOCNOISE>',\n",
       " 'want',\n",
       " 'make',\n",
       " 'the',\n",
       " 'about',\n",
       " 'yknow',\n",
       " 'when',\n",
       " 'not',\n",
       " 'flips',\n",
       " 'gonna',\n",
       " '<SIL>',\n",
       " 'game',\n",
       " 'no',\n",
       " '<SIL>',\n",
       " 'see',\n",
       " '<SIL>',\n",
       " '<SIL>',\n",
       " 'and',\n",
       " 'you',\n",
       " '<IVER>',\n",
       " 'i',\n",
       " 'the',\n",
       " 'mentality',\n",
       " 'just',\n",
       " 'the',\n",
       " 'um-hum',\n",
       " 'lot',\n",
       " 'while',\n",
       " 'have',\n",
       " 'the',\n",
       " 'of',\n",
       " 'knew',\n",
       " '<VOCNOISE>',\n",
       " '<VOCNOISE>',\n",
       " 'um',\n",
       " 'least',\n",
       " '<SIL>',\n",
       " 'yknow']"
      ]
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2vecs were trained with 'words' being hexadecimal notation of Kmeans cluster IDs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "source": [
    "def decode_encodings_w2v(clusterId, model, c2vDict, topn=5):\n",
    "    simHexes = model.wv.most_similar(hex(clusterId), topn=topn)\n",
    "    decodedHexes = [(int(s, 16), c2vDict[int(s, 16)]) for (s, _) in simHexes]\n",
    "    print(f\"Queried Cluster ID: {clusterId}\\n\")\n",
    "    print(f\"Corresponding unique word types assigned to it: {[x[0] for x in groupby(c2vDict[clusterId])]}\\n\")\n",
    "    print(f\"Top {topn} matching clusters (with corresponding unique word types assigned to it):\")\n",
    "    for (c, ws) in decodedHexes:\n",
    "        print(c)\n",
    "        print([x[0] for x in groupby(ws)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "w2v_10000 = Word2Vec.load(str(path_w2v_10000 + '/w2v.model'))\n",
    "w2v_10000_weighted = Word2Vec.load(str(path_w2v_10000_weighted + '/w2v.model'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "source": [
    "Id = 88\n",
    "decode_encodings_w2v(Id, w2v_10000, k10000_cluster2WordDict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Queried Cluster ID: 88\n",
      "\n",
      "Corresponding unique word types assigned to it: ['<SIL>', 'all', 'and', 'um-hum', 'always', 'was', 'look', 'along', 'everybody', '<VOCNOISE>', 'hold', '<SIL>', '<VOCNOISE>', 'brother', 'the', 'these', 'when', 'a', \"i've\", 'but', 'being', 'um']\n",
      "\n",
      "Top 5 matching clusters (with corresponding unique word types assigned to it):\n",
      "8387\n",
      "['<VOCNOISE>', '<SIL>', '<VOCNOISE>', '<IVER>', '<SIL>', 'of', '<SIL>', 'stuff', '<VOCNOISE>', '<SIL>', \"isn't\", '<SIL>', 'everything', 'world', 'because', '<SIL>', 'expanding', 'the', 'um-hum', 'a', 'after', 'yeah', 'was', 'rudest', 'my', '<SIL>', \"isn't\", 'these', 'great', 'schools', 'so', 'think', 'but', 'hold', 'her', 'to', 'in', 'ground', 'people']\n",
      "1349\n",
      "['either', 'trimester', 'my', 'planes', 'time', 'sometimes', 'downstairs', 'times', 'the', 'brand', 'and', 'transmeridian', 'apartment', 'i', 'times', 'um-hum', 'higher', 'program', 'have', '<VOCNOISE>', 'side', '<SIL>', 'store', '<IVER>', 'about', 'over', \"don't\", '<LAUGH>', 'thing', '<IVER>', '<SIL>', 'things', 'way', 'kind', 'i', 'been', '<SIL>', 'right', 'a', 'because', 'think', 'this', 'they', 'to', 'just', 'and', 'too', 'reading', 'way', 'and', 'is', 'into', 'to', '<VOCNOISE>', 'real', 'had', 'anything', 'but', 'not', 'with', 'gender', '<VOCNOISE>', '<SIL>', 'the']\n",
      "8047\n",
      "['<IVER>', '<SIL>', '<IVER>', '<SIL>', 'it', 'through', 'fog', 'to', 'i', '<VOCNOISE>', 'makes', '<VOCNOISE>', 'can', '<SIL>', 'um', 'there']\n",
      "8691\n",
      "['<NOISE>', '<IVER>', '{E_TRANS}', '<SIL>', 'i', '<IVER>', '<SIL>', 'a', '<SIL>', '<IVER>', '<SIL>', '<IVER>', '<SIL>', 'wipers', 'that', 'in', 'she', 'going', 'us', '<SIL>', 'on', '<SIL>', 'go', 'uh', 'to', '<SIL>', 'they', '<SIL>', 'we', 'flaky', 'something', '<SIL>', 'standpoint', 'that', 'were', 'uh', '<VOCNOISE>', 'is']\n",
      "1624\n",
      "['i', '<IVER>', 'a', 'guy', 'i', '<SIL>', 'i', 'uh', 'i', 'and', \"don't\", 'i', 'if', 'that', 'the', 'yeah', 'stuff', 'my', 'i', 'house', 'just', 'off', 'in', 'take', 'kinda', 'the', 'their', '<SIL>', 'not', 'it', 'school', 'obviously', '<SIL>', 'top', 'just', '<VOCNOISE>', 'and', '<SIL>', 'police', 'did', 'that', 'it', 'yknow', 'yeah', 'a', 'stuff', 'invest', 'helped', 'person']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "decode_encodings_w2v(Id, w2v_10000_weighted, k10000_weighted_cluster2WordDict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Queried Cluster ID: 88\n",
      "\n",
      "Corresponding unique word types assigned to it: ['<IVER>', 'own', '<VOCNOISE>', 'me', \"you're\", 'then', 'so', 'kinda', 'that', 'are', 'time', 'things', 'principles', '<SIL>', 'back', 'what', 'because', 'there', 'wanna', 'and', 'to', 'on', 'social', '<SIL>', 'be', '<VOCNOISE>', 'my', 'you', 'she', 'for', 'man', 'and']\n",
      "\n",
      "Top 5 matching clusters (with corresponding unique word types assigned to it):\n",
      "873\n",
      "['they', '<VOCNOISE>', 'serious', 'really', 'it', 'whole', 'for', 'and', 'up', 'thirty', 'you', '<VOCNOISE>', 'talk', 'resources', 'of', 'think', 'need', 'can', 'just', 'me', '<SIL>', 'and', 'dad', 'time', 'voted', 'election', 'sit', 'we', \"there's\", '<SIL>', 'should', 'like', '<IVER>', 'in', 'faith', 'know', '<SIL>', '<IVER>', 'what', 'cool', 'integrated', 'that', 'nobody', '<IVER>', 'here', 'kind', 'i', 'um-hmm', 'where', '<IVER>', \"didn't\", 'but', 'bad', '<SIL>', 'and', 'think', '<IVER>', 'apparently', 'in', 'it', 'and', 'the', 'just', '<VOCNOISE>', 'every', 'my', 'dollars']\n",
      "6788\n",
      "['<VOCNOISE>', 'scott', 'all', 'just', 'it', 'i', '<SIL>', 'they', \"it's\", '<SIL>', \"don't\", 'level', '<VOCNOISE>', 'd', 'some', 'really', '<SIL>', '<IVER>', 'that', 'uh', '<VOCNOISE>', \"we're\", 'of']\n",
      "4679\n",
      "['<SIL>', 'get', 'here', 'for', 'does', '<SIL>', 'certainly', \"couldn't\", 'it', 'women', 'he', 'like', 'through', \"wasn't\", 'some', '<VOCNOISE>', '<IVER>', 'i', 'um', 'uh', 'to', 'that', 'yknow', \"weren't\", 'was', 'believe', 'father']\n",
      "164\n",
      "[\"don't\", 'the', 'put', 'together', \"you've\", 'and', 'three', '<SIL>', 'take', 'and', 'um-hum', 'found', 'or', 'on', 'of', 'because', \"i'd\", \"don't\", 'out', 'a', 'and', 'rode', 'the']\n",
      "1080\n",
      "['up', 'plop', 'fort', 'day', 'gonna', 'will', '<SIL>', 'to', 'words', '<VOCNOISE>', 'thing', 'zombie', 'like', 'they', 'thing', 'yknow', 'like', 'the', '<IVER>']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K = 11000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "k11000, k11000_preds, k11000_cluster2WordDict = get_kmeans_results(pathKMeans_11000, wordEncs, uniqueConsecutiveLabels, preds=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "k11000_weighted, k11000_weighted_preds, k11000_weighted_cluster2WordDict = get_kmeans_results(pathKMeans_11000_weighted, wordEncs, uniqueConsecutiveLabels, preds=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "w2v_11000 = Word2Vec.load(str(path_w2v_11000 + '/w2v.model'))\n",
    "w2v_11000_weighted = Word2Vec.load(str(path_w2v_11000_weighted + '/w2v.model'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "Id = 97\n",
    "decode_encodings_w2v(Id, w2v_11000, k11000_weighted_cluster2WordDict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Queried Custer ID: 97\n",
      "\n",
      "Corresponding word types assigned to it: ['<SIL>', '<VOCNOISE>', '<IVER>', '<VOCNOISE>', '<SIL>', '<VOCNOISE>', '<IVER>', '<SIL>', 'and', '<SIL>', '<IVER>', '<SIL>', 'there', 'a']\n",
      "\n",
      "Top 5 matching clusters (with corresponding unique word types assigned to it):\n",
      "7429\n",
      "[\"grandfather's\", 'other', 'any', 'right', 'do', 'uh', 'believe', 'age', 'anywhere', 'would', 'mind', 'not', 'england', 'mean', 'has', 'that', 'uh', '<VOCNOISE>', 'how', 'balsamic', 'kind', '<SIL>', 'i', 'climbing', 'yeah', 'out', 'grew', 'when', '<VOCNOISE>', 'positions', 'is', 'here', 'our', 'was', 'apartment', 'delivered', 'i', 'and', 'she', 'i', 'a', 'child', 'now', 'but', 'and', 'right', '<NOISE>', '<SIL>', \"didn't\", 'seventies', 'uh', 'even', 'an', 'marriage', '<VOCNOISE>', 'yeah', 'and', 'connecticut', 'its', 'way', 'she', 'as', 'just', 'not', 'me', 'too', 'easy', 'just', 'yknow', 'their', '<VOCNOISE>', 'she', 'a', 'was', 'of', 'yeah', \"i'm\", 'i', 'of', 'and', 'they', '<SIL>', '<VOCNOISE>', 'okay', 'it', 'things', 'let', 'and', \"they're\", 'yknow', '<VOCNOISE>', 'so', '<VOCNOISE>', 'yknow', 'went', '<VOCNOISE>', 'slacks', 'be', '<SIL>', '<IVER>']\n",
      "2116\n",
      "['<VOCNOISE>', '<IVER>']\n",
      "5427\n",
      "['use', 'there', 'getting', 'things', 'yknow', 'in', 'and', 'things', 'yknow', 'need', 'gotten', 'thing', 'here', 'gonna', 'saying', 'amount', 'like', 'my', '<SIL>', 'the', 'how', 'these', 'all', 'fifty', '<SIL>', '<VOCNOISE>', 'so', '<VOCNOISE>', 'for', 'to', 'death', 'like']\n",
      "8465\n",
      "['<SIL>', 'yknow', 'job', \"didn't\", 'the', 'are', 'anything', 'know', '<SIL>', \"it's\", 'it', 'a', 'i', 'least', 'i', '<VOCNOISE>', 'so', 'uh']\n",
      "3460\n",
      "['paul', 'have', 'us', '<VOCNOISE>', '<SIL>', 'and', '<VOCNOISE>', 'where', 'go', 'i', 'like', 'if', 'money', 'easier', 'two', 'going', 'playing', 'was', 'wife', 'we', 'i', '<SIL>', 'things']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('cpc37': conda)"
  },
  "interpreter": {
   "hash": "55aaedaa7f03cec5e6f90f6a6eff5bd43e77e241b961792e7851190b764a2f7b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}